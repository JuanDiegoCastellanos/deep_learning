{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Castessoft_recurrent_neural_networks_(PLN)_BibliaReinaValera1909.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00X35jljBrXA"
      },
      "source": [
        "# GENERADOR DE ESCRITOS SAGRADOS DE LA SAGRADA BIBLIA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_aK4gzoDWrc"
      },
      "source": [
        "## 1. Instalar librerías cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z1vApDUBB0R",
        "outputId": "913fc8fd-1e70-4740-f20b-bad5da047e81"
      },
      "source": [
        "#!pip list\n",
        "!nvcc --version  #Version de CUDA en la maquina virtual"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pxOKBogDylc"
      },
      "source": [
        "## 2. Importacion de librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJyaUK9YD1Kv"
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit               #para medir tiempos\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3XNcJISEEa6"
      },
      "source": [
        "## 3. Uso de GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x51_WD0EEJRb",
        "outputId": "5fa224b5-c809-4693-f2cd-d22bf9bf4c07"
      },
      "source": [
        "print(\"Tensorflow Version: \", tf.__version__)\n",
        "print(\"Dispositivos disponibles para entrenar: \", tf.config.list_physical_devices())\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Encontrada la GPU: {}'.format(device_name))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Version:  2.7.0\n",
            "Dispositivos disponibles para entrenar:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Encontrada la GPU: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFYHxN1eE8Qp"
      },
      "source": [
        "## 4. GPU VS CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm3IVUjmFABc"
      },
      "source": [
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_WX6kMUFFe8",
        "outputId": "380a4ed3-30b8-498c-cb99-f7b610fadcce"
      },
      "source": [
        "cpu()  #ejecutamos entrenamiento con CPU\n",
        "gpu()  #ejecutamos entrenamiento con GPU\n",
        "# Run the op several times.\n",
        "print('TIEMPO (seg) para entrenar una red convolucional de 32x7x7x3 filtros sobre un randomico de 100x100x100x3 imagenes '\n",
        "      '(batch x height x width x channel). suma de 10 epochs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TIEMPO (seg) para entrenar una red convolucional de 32x7x7x3 filtros sobre un randomico de 100x100x100x3 imagenes (batch x height x width x channel). suma de 10 epochs.\n",
            "CPU (s):\n",
            "3.785504455000023\n",
            "GPU (s):\n",
            "0.05223093699999026\n",
            "GPU speedup over CPU: 72x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imfzbW_4FLua"
      },
      "source": [
        "## 5. Dejando activa la GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrs0g-ECFLFx",
        "outputId": "7dab0ad9-bb41-424d-c77d-61ab21cf3c7b"
      },
      "source": [
        "#tf.device('/gpu:0') #activando la CPU\n",
        "tf.device('/device:GPU:0') #activando la GPU"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.context._EagerDeviceContext at 0x7fa4c8bba780>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HNzOxO-FTJI"
      },
      "source": [
        "## 6. Descarga y preproceso de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbsdLm9JF3UB",
        "outputId": "d8a6b344-e087-4297-b5f6-90e610653e5d"
      },
      "source": [
        "#https://raw.githubusercontent.com/JuanDiegoCastellanos/deep_learning/main/Datasets/Biblia_Reina-Valera_1909.txt\n",
        "fileDL= tf.keras.utils.get_file('Biblia_Reina-Valera_1909.txt','https://raw.githubusercontent.com/JuanDiegoCastellanos/deep_learning/main/Datasets/Biblia_Reina-Valera_1909.txt')\n",
        "texto = open(fileDL, 'rb').read().decode(encoding='utf-8')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/JuanDiegoCastellanos/deep_learning/main/Datasets/Biblia_Reina-Valera_1909.txt\n",
            "3104768/3096688 [==============================] - 0s 0us/step\n",
            "3112960/3096688 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P22-4dDGFK_"
      },
      "source": [
        "### 6.1 Normalización del texto\n",
        "1. pasar todo a minusculas\n",
        "2. convertir tildes a vocales sin tilde\n",
        "3. eliminar caracteres especiales (*#!()%=)\n",
        "4. eliminar saltos de linea y tab (identación)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl1JdiilGVK2"
      },
      "source": [
        "import re\n",
        "from unicodedata import normalize\n",
        "#pasa todo a minuscula\n",
        "texto     = texto.lower()\n",
        "#reemplazar tildes por letras similares sin tildes\n",
        "transfor  = dict.fromkeys(map(ord, u'\\u0301\\u0308'), None)\n",
        "texto     = normalize('NFKC', normalize('NFKD', texto).translate(transfor))\n",
        "#quitar saltos de linea\n",
        "texto      = texto.strip()\n",
        "texto      = re.sub('\\r|\\n', ' ',texto)\n",
        "#quitar espacios dobles\n",
        "texto      = re.sub(' +', ' ', texto)\n",
        "#quitando caracteres especiales\n",
        "texto = re.sub(r\"[^a-zA-Z0-9]+\",\" \",texto)\n",
        "print(texto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTzjmyoaGm1V"
      },
      "source": [
        "## 7. Comprendiendo el texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnOApgxeGyjk",
        "outputId": "db2a5f50-f1bc-443f-dc14-654d8966a575"
      },
      "source": [
        "print('el texto tiene longitud de:{} caracteres'. format(len(texto)))\n",
        "vocab = sorted(set(texto))\n",
        "print('el texto esta compuesto de estos :{} caracteres'. format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el texto tiene longitud de:2876717 caracteres\n",
            "el texto esta compuesto de estos :36 caracteres\n",
            "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRMeBwoWHD_1"
      },
      "source": [
        "## 8. Convertir Texto en Número\n",
        "Las redes neuronales solo procesan valores numéricos, no letras, por tanto tenemos que traducir los caracteres a representación numérica.  Para ello crearemos dos “tablas de traducción”: una de caracteres a  números y otra de números a caracteres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Vo0avxHHdG",
        "outputId": "a9a75b24-b264-4cc1-ba34-0649fc8119b1"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)} # asignamos un número a cada vocablo\n",
        "idx2char = np.array(vocab)\n",
        "#-----------revisando las conversiones\n",
        "\n",
        "#for char,_ in zip(char2idx, range(len(vocab))):\n",
        "    #print(' {:4s}: {:3d},'.format(repr(char),char2idx[char]))\n",
        "\n",
        "#pasamos todo el texto a números\n",
        "texto_como_entero= np.array([char2idx[c] for c in texto])\n",
        "print('texto: {}'.format(repr(texto[:100])))\n",
        "print('{}'.format(repr(texto_como_entero[:100])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texto: 'biblia reina valera 1909 1 en el principio crio dios los cielos y la tierra 2 y la tierra estaba des'\n",
            "array([12, 19, 12, 22, 19, 11,  0, 28, 15, 19, 24, 11,  0, 32, 11, 22, 15,\n",
            "       28, 11,  0,  2, 10,  1, 10,  0,  2,  0, 15, 24,  0, 15, 22,  0, 26,\n",
            "       28, 19, 24, 13, 19, 26, 19, 25,  0, 13, 28, 19, 25,  0, 14, 19, 25,\n",
            "       29,  0, 22, 25, 29,  0, 13, 19, 15, 22, 25, 29,  0, 34,  0, 22, 11,\n",
            "        0, 30, 19, 15, 28, 28, 11,  0,  3,  0, 34,  0, 22, 11,  0, 30, 19,\n",
            "       15, 28, 28, 11,  0, 15, 29, 30, 11, 12, 11,  0, 14, 15, 29])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0W-SFa1HiDF"
      },
      "source": [
        "## 9. Exportar vocablos y matriz númerica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVyziJFtH1WV"
      },
      "source": [
        "rows=[]\n",
        "columns=['num','vocab']\n",
        "for i, voc in enumerate(vocab):\n",
        "  #print(i,'-->', voc)\n",
        "  rows.append([i,voc])\n",
        "df= pd.DataFrame(columns=['num','vocab'],data=rows)\n",
        "df.head(10)\n",
        "df.to_csv('data_vocab.csv',index=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k16giZy6H5kU"
      },
      "source": [
        "## 10. Preparar Datos para la RNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qmoq34SICWg",
        "outputId": "e67d5238-3f42-4b71-c150-c10914b32538"
      },
      "source": [
        "char_dataset= tf.data.Dataset.from_tensor_slices(texto_como_entero)\n",
        "#cantidad de secuencia de caracteres\n",
        "secu_length=50\n",
        "#creamos secuencias de maximo 100 caractereres\n",
        "secuencias= char_dataset.batch(secu_length+1, drop_remainder=True)\n",
        "for item in secuencias.take(10):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'biblia reina valera 1909 1 en el principio crio dio'\n",
            "'s los cielos y la tierra 2 y la tierra estaba desor'\n",
            "'denada y vacia y las tinieblas estaban sobre la haz'\n",
            "' del abismo y el espiritu de dios se movia sobre la'\n",
            "' haz de las aguas 3 y dijo dios sea la luz y fue la'\n",
            "' luz 4 y vio dios que la luz era buena y aparto dio'\n",
            "'s la luz de las tinieblas 5 y llamo dios a la luz d'\n",
            "'ia y a las tinieblas llamo noche y fue la tarde y l'\n",
            "'a ma ana un dia 6 y dijo dios haya expansion en med'\n",
            "'io de las aguas y separe las aguas de las aguas 7 e'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQhSJvGuIG70"
      },
      "source": [
        "## 11. Separar datos en agrupamientos (BATCHES)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4JDPulDINEQ",
        "outputId": "120718fc-4434-42f4-f45f-cb14498c53c4"
      },
      "source": [
        "#funcion para obtener el conjunto de datos de trainning\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text= chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset  = secuencias.map(split_input_target)\n",
        "#el dataset contiene un conjunto de parejas de secuencia de texto\n",
        "#(con la representación numérica de los caracteres), donde el \n",
        "#primer componente de la pareja contiene un paquete con una secuencia \n",
        "#de 100 caracteres del texto original y la segunda su correspondiente salida, \n",
        "#también de 100 caracteres. )\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print('input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target data: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input data:  'biblia reina valera 1909 1 en el principio crio di'\n",
            "Target data:  'iblia reina valera 1909 1 en el principio crio dio'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c6aGi3sISe0"
      },
      "source": [
        "## 12. Mostrar el Tensor del DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgzUhkQsIYUe",
        "outputId": "1503dff8-a216-4ae9-87e8-591c7ab18248"
      },
      "source": [
        "#imprimimos el tensor del dataset\n",
        "print(dataset)\n",
        "#Hyper-Parametros para entrenamiento  de una rede neuronal \n",
        "#   -los datos se agrupan en batch\n",
        "BATCH_SIZE= 64\n",
        "#    -Tamaño de memoria disponible \n",
        "BUFFER_SIZE=10000\n",
        "dataset= dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print (dataset)\n",
        "#En el tensor dataset disponemos los datos de entrenamiento\n",
        "#con agrupamienttos (batches) compuestos de 64 parejas de secuencias \n",
        "#de 100 integers de 64 bits que representan el carácter correspondiente \n",
        "#en el vocabulario."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MapDataset shapes: ((50,), (50,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((64, 50), (64, 50)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPgA8wBIaf4"
      },
      "source": [
        "## 13. Construcción de Modelo RNN\n",
        "Para construir el modelo usaremos tf.keras.Sequential. Usaremos una versión mínima de RNN, que contenga solo una capa LSTM y 3 capas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4cMSH3kImbB",
        "outputId": "e8e6731e-df53-4ccb-d0b5-2fd8be54c547"
      },
      "source": [
        "#como es un problema de clasificación estándar \n",
        "#para el que debemos definir la función de Lossy el optimizador.\n",
        "def lossy(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "def create_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  #creando el modelo\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                               \n",
        "  ])\n",
        "  #En cuanto al optimizador usaremos tf.keras.optimizers.Adam \n",
        "  #con los argumentos por defecto del optimizador Adam. \n",
        "  model.compile(optimizer='adam',\n",
        "              loss='',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_model_two(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  #creando el modelo\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                               \n",
        "  ])\n",
        "  #En cuanto al optimizador usaremos tf.keras.optimizers.Adam \n",
        "  #con los argumentos por defecto del optimizador Adam. \n",
        "  model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "vocab_size= len(vocab)\n",
        "#dimensiones de los vectores que tendrá la capa.\n",
        "embedding_dim= 256\n",
        "#cantidad de neuronas\n",
        "rnn_units=1024\n",
        "#creamos nuestra red neuronal RNN\n",
        "model=create_model(vocab_size   =vocab_size,\n",
        "                  embedding_dim =embedding_dim,\n",
        "                  rnn_units     =rnn_units,\n",
        "                  batch_size    =BATCH_SIZE)\n",
        "#summary()para visualizar la estructura del modelo\n",
        "model.summary()\n",
        "#resultados=  -La capa LSTM consta más de 5 millones de parametros)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (64, None, 256)           9216      \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (64, None, 36)            36900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,293,092\n",
            "Trainable params: 5,293,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRTuNPozIsQ5"
      },
      "source": [
        "### 13.1 Creación de checkpoints\n",
        "una técnica de tolerancia de fallos para procesos cuyo tiempo de ejecución es muy largo. La idea es guardar una instantánea del estado del sistema periódicamente para recuperar desde ese punto la ejecución en caso de fallo del sistema.\n",
        "\n",
        "los crearemos en google drive para mejorar la capacidad de reentrenamiento de la red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbOsp8FBIwZp",
        "outputId": "5274635f-e5fc-446d-ba15-44960049509e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZe46z9sJBZi"
      },
      "source": [
        "checkpoint_dir='/content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2'\n",
        "checkpoint_prefix= os.path.join(checkpoint_dir,\"cp_{epoch:04d}.ckpt\")\n",
        "\n",
        "\n",
        "cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                               monitor='loss',\n",
        "                                               verbose=1,\n",
        "                                               save_weights_only=True,\n",
        "                                               save_best_only=True,\n",
        "                                               mode='auto')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG79T8-OJSFi"
      },
      "source": [
        "## 14. Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku_bWrbuJU54"
      },
      "source": [
        "### 14.1 Entrenando para usar checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N93NIxVtJZFh",
        "outputId": "3f5b1547-e70e-495b-ee20-60cc56698660"
      },
      "source": [
        "EPOCHS=20\n",
        "history=model.fit(dataset, \n",
        "                  epochs=EPOCHS, \n",
        "                  verbose=1,\n",
        "                  callbacks=[cp_callback])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 1.6355 - accuracy: 0.4987\n",
            "Epoch 00001: loss improved from inf to 1.63553, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0001.ckpt\n",
            "881/881 [==============================] - 83s 91ms/step - loss: 1.6355 - accuracy: 0.4987\n",
            "Epoch 2/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 1.2320 - accuracy: 0.6143\n",
            "Epoch 00002: loss improved from 1.63553 to 1.23205, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0002.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 1.2320 - accuracy: 0.6143\n",
            "Epoch 3/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 1.1603 - accuracy: 0.6351\n",
            "Epoch 00003: loss improved from 1.23205 to 1.16031, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0003.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 1.1603 - accuracy: 0.6351\n",
            "Epoch 4/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 1.1197 - accuracy: 0.6469\n",
            "Epoch 00004: loss improved from 1.16031 to 1.11968, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0004.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 1.1197 - accuracy: 0.6469\n",
            "Epoch 5/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 1.0898 - accuracy: 0.6562\n",
            "Epoch 00005: loss improved from 1.11968 to 1.08982, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0005.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 1.0898 - accuracy: 0.6562\n",
            "Epoch 6/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 1.0650 - accuracy: 0.6636\n",
            "Epoch 00006: loss improved from 1.08982 to 1.06498, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0006.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 1.0650 - accuracy: 0.6636\n",
            "Epoch 7/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 1.0433 - accuracy: 0.6707\n",
            "Epoch 00007: loss improved from 1.06498 to 1.04332, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0007.ckpt\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 1.0433 - accuracy: 0.6707\n",
            "Epoch 8/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 1.0235 - accuracy: 0.6768\n",
            "Epoch 00008: loss improved from 1.04332 to 1.02352, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0008.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 1.0235 - accuracy: 0.6768\n",
            "Epoch 9/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 1.0060 - accuracy: 0.6828\n",
            "Epoch 00009: loss improved from 1.02352 to 1.00601, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0009.ckpt\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 1.0060 - accuracy: 0.6828\n",
            "Epoch 10/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9896 - accuracy: 0.6883\n",
            "Epoch 00010: loss improved from 1.00601 to 0.98959, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0010.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.9896 - accuracy: 0.6883\n",
            "Epoch 11/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9750 - accuracy: 0.6930\n",
            "Epoch 00011: loss improved from 0.98959 to 0.97504, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0011.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.9750 - accuracy: 0.6930\n",
            "Epoch 12/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9613 - accuracy: 0.6977\n",
            "Epoch 00012: loss improved from 0.97504 to 0.96127, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0012.ckpt\n",
            "881/881 [==============================] - 83s 92ms/step - loss: 0.9613 - accuracy: 0.6977\n",
            "Epoch 13/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9496 - accuracy: 0.7018\n",
            "Epoch 00013: loss improved from 0.96127 to 0.94958, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0013.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.9496 - accuracy: 0.7018\n",
            "Epoch 14/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9387 - accuracy: 0.7055\n",
            "Epoch 00014: loss improved from 0.94958 to 0.93871, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0014.ckpt\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.9387 - accuracy: 0.7055\n",
            "Epoch 15/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9295 - accuracy: 0.7088\n",
            "Epoch 00015: loss improved from 0.93871 to 0.92950, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0015.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.9295 - accuracy: 0.7088\n",
            "Epoch 16/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9212 - accuracy: 0.7116\n",
            "Epoch 00016: loss improved from 0.92950 to 0.92119, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0016.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.9212 - accuracy: 0.7116\n",
            "Epoch 17/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9136 - accuracy: 0.7141\n",
            "Epoch 00017: loss improved from 0.92119 to 0.91356, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0017.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.9136 - accuracy: 0.7141\n",
            "Epoch 18/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9067 - accuracy: 0.7167\n",
            "Epoch 00018: loss improved from 0.91356 to 0.90674, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0018.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.9067 - accuracy: 0.7167\n",
            "Epoch 19/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9013 - accuracy: 0.7185\n",
            "Epoch 00019: loss improved from 0.90674 to 0.90130, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0019.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.9013 - accuracy: 0.7185\n",
            "Epoch 20/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8966 - accuracy: 0.7203\n",
            "Epoch 00020: loss improved from 0.90130 to 0.89661, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0020.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.8966 - accuracy: 0.7203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqxGdu1Jlh5"
      },
      "source": [
        "### 14.2 Entrenando desde un checkpoint\n",
        "\n",
        "\n",
        "Desde la carpeta que optamos guardar los checkpoints\n",
        "\n",
        "- El archivo .data es el archivo que contiene nuestras variables de entrenamiento y vamos a ir tras él.\n",
        "- El archivo checkpoint, simplemente mantiene un registro de los últimos archivos de punto de control guardados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GecEgggyJ0B4",
        "outputId": "c0745263-f47d-4903-e943-8b2cf321f7b5"
      },
      "source": [
        "#creamos un modelo con iguales caracteristicas al 1° modelo\n",
        "model=create_model(vocab_size   =vocab_size,\n",
        "                  embedding_dim =embedding_dim,\n",
        "                  rnn_units     =rnn_units,\n",
        "                  batch_size    =BATCH_SIZE)\n",
        "\n",
        "#buscamos el ultimo checkpoint de entrenamiento\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "print(latest)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0020.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUsbq_K_KEv9"
      },
      "source": [
        "### 14.3 Cargando los pesos y entrenando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9eY9zF8KOpD",
        "outputId": "c6bc7a40-c52a-45d7-d9f9-bdbec0b11100"
      },
      "source": [
        "# cargamos los pesos al nuevo modelo (estos valores tienes una variación de un 10%)\n",
        "model.load_weights(latest)\n",
        "# continuamos el entrenamiento desde el checkpoint en que quedamos\n",
        "history2=model.fit(dataset, \n",
        "                    epochs=20, \n",
        "                    verbose=1,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8937 - accuracy: 0.7213\n",
            "Epoch 00001: loss improved from 0.89661 to 0.89371, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0001.ckpt\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8937 - accuracy: 0.7213\n",
            "Epoch 2/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8907 - accuracy: 0.7224\n",
            "Epoch 00002: loss improved from 0.89371 to 0.89073, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0002.ckpt\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8907 - accuracy: 0.7224\n",
            "Epoch 3/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8882 - accuracy: 0.7230\n",
            "Epoch 00003: loss improved from 0.89073 to 0.88825, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0003.ckpt\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8882 - accuracy: 0.7230\n",
            "Epoch 4/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8876 - accuracy: 0.7236\n",
            "Epoch 00004: loss improved from 0.88825 to 0.88761, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0004.ckpt\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8876 - accuracy: 0.7236\n",
            "Epoch 5/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8852 - accuracy: 0.7243\n",
            "Epoch 00005: loss improved from 0.88761 to 0.88515, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0005.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.8852 - accuracy: 0.7243\n",
            "Epoch 6/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8847 - accuracy: 0.7243\n",
            "Epoch 00006: loss improved from 0.88515 to 0.88472, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0006.ckpt\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8847 - accuracy: 0.7243\n",
            "Epoch 7/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8845 - accuracy: 0.7245\n",
            "Epoch 00007: loss improved from 0.88472 to 0.88454, saving model to /content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/checkpointsV2/cp_0007.ckpt\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.8845 - accuracy: 0.7245\n",
            "Epoch 8/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8846 - accuracy: 0.7242\n",
            "Epoch 00008: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 82s 91ms/step - loss: 0.8846 - accuracy: 0.7242\n",
            "Epoch 9/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8864 - accuracy: 0.7238\n",
            "Epoch 00009: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8864 - accuracy: 0.7238\n",
            "Epoch 10/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8871 - accuracy: 0.7236\n",
            "Epoch 00010: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8871 - accuracy: 0.7236\n",
            "Epoch 11/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8883 - accuracy: 0.7231\n",
            "Epoch 00011: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 82s 91ms/step - loss: 0.8883 - accuracy: 0.7231\n",
            "Epoch 12/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8913 - accuracy: 0.7218\n",
            "Epoch 00012: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8913 - accuracy: 0.7218\n",
            "Epoch 13/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8946 - accuracy: 0.7207\n",
            "Epoch 00013: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8946 - accuracy: 0.7207\n",
            "Epoch 14/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8962 - accuracy: 0.7199\n",
            "Epoch 00014: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 82s 92ms/step - loss: 0.8962 - accuracy: 0.7199\n",
            "Epoch 15/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.8985 - accuracy: 0.7191\n",
            "Epoch 00015: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.8985 - accuracy: 0.7191\n",
            "Epoch 16/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9027 - accuracy: 0.7175\n",
            "Epoch 00016: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.9027 - accuracy: 0.7175\n",
            "Epoch 17/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9059 - accuracy: 0.7165\n",
            "Epoch 00017: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.9059 - accuracy: 0.7165\n",
            "Epoch 18/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9102 - accuracy: 0.7151\n",
            "Epoch 00018: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.9102 - accuracy: 0.7151\n",
            "Epoch 19/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.7138\n",
            "Epoch 00019: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.9138 - accuracy: 0.7138\n",
            "Epoch 20/20\n",
            "881/881 [==============================] - ETA: 0s - loss: 0.9185 - accuracy: 0.7118\n",
            "Epoch 00020: loss did not improve from 0.88454\n",
            "881/881 [==============================] - 81s 91ms/step - loss: 0.9185 - accuracy: 0.7118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIOsPMJ4xQbC"
      },
      "source": [
        "## 15. Generando texto nuevo usando la RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O9hqNdOxpaO"
      },
      "source": [
        "#creamos un modelo tomando como base el ultimo checkpoint\n",
        "model = create_model_two(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJH06lm7xrmH"
      },
      "source": [
        "#funcion para generar texto\n",
        "def generate_text(model, start_string):\n",
        "  #definimos cuantos tensores/cantidad de texto generaremos\n",
        "  num_generate=500\n",
        "  #convertimos el texto en números\n",
        "  input_eval=[char2idx[s] for s in start_string]\n",
        "  input_eval= tf.expand_dims (input_eval,0)\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 0.2  #(0.0 a  1) entre más alta la temperatura más creatividad al modelo, pero tambien más errores ortograficos.\n",
        "  model.reset_states() #bucle para generar caracteres, mediante predicciones\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval= tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append (idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string+ ''.join(text_generated))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ZxR3TCxwMd"
      },
      "source": [
        "## 15.1 Generando Escritos sagrados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNf1IOE9x08U",
        "outputId": "31cbc366-fec6-4479-d1d4-51219d3891ed"
      },
      "source": [
        "print(generate_text(model, start_string=u\"los diez mandamientos\"))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "los diez mandamientos de jehova y de la tierra de egipto y las casas de sus padres y la poseeran segun sus estatutos y sus vestimentas y sus testimonios y sus mujeres a sus siervos y sus principes y sus hijas y las palabras de jehova son los que se apartan para siempre 1 y fue a mi palabra de jehova diciendo 2 hijo del hombre no quiere de aquel que se ha de parto 16 y envio jehova a moises y a aaron y a sus hijos y a toda la tierra de egipto y la tierra se esconde en la tierra de egipto 10 y la higuera que haga conf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIMB0tKR0A3u"
      },
      "source": [
        "## 16. Exportando Modelo\n",
        "Guardamos y Serializamos el Modelo (con esto ya podemos vender nuestro modelo de predicción de texto según lo aprendido por nuestra RNN)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0XHh-3Z0J2u",
        "outputId": "52c2f9a3-dcac-4ec1-ae41-27136bf70fb6"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "import os\n",
        "dir_export= '/content/gdrive/MyDrive/Colab Notebooks/RedesNeuronalesRecurrentes/Modelos'\n",
        "#dir_export= os.path.join(dir_drive)\n",
        "# Serializamos el modelo en forma JSON\n",
        "model_json = model.to_json()\n",
        "with open(os.path.join(dir_export,'2RNN_BibliaReinaValera1909_json.json'), 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(os.path.join(dir_export,'2RNN_BibliaReinaValera1909_pesos.hdf5'))\n",
        "model.save(os.path.join(dir_export,'2RNN_BibliaReinaValera1909_model.h5'))\n",
        "print(\"modelo salvado en Drive de google\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modelo salvado en Drive de google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTIXFQcz0u6X"
      },
      "source": [
        "## 17. Cargar modelo serializado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-6rUJ0e0zN1"
      },
      "source": [
        "### 17.1 Descargar modelo usando wget"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTNH_bu95Lwg",
        "outputId": "3f0724de-2387-425e-fc2e-289f4c38b04b"
      },
      "source": [
        "!wget https://github.com/JuanDiegoCastellanos/deep_learning/blob/main/neural_recurrent_network/models/2RNN_BibliaReinaValera1909_model.h5 \\\n",
        "      -O 2RNN_BibliaReinaValera1909_model.h5"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-23 23:22:13--  https://github.com/JuanDiegoCastellanos/deep_learning/blob/main/neural_recurrent_network/models/2RNN_BibliaReinaValera1909_model.h5\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘2RNN_BibliaReinaValera1909_model.h5’\n",
            "\n",
            "2RNN_BibliaReinaVal     [ <=>                ] 163.21K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-11-23 23:22:13 (2.12 MB/s) - ‘2RNN_BibliaReinaValera1909_model.h5’ saved [167129]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9_cxjns5kab"
      },
      "source": [
        "### 17.2 Descargamo el modelo usando PYRIND & URLLIB (OPCIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVThkTmH50v7",
        "outputId": "c0909414-341b-4091-ec53-de0560bf4fff"
      },
      "source": [
        "!pip install pyprind"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.7/dist-packages (2.11.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZG5We_S553U",
        "outputId": "e687678a-3344-4822-cf39-d9ae4267e1f9"
      },
      "source": [
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = progress_size / (1024.**2 * duration)\n",
        "    percent = count * block_size * 100. / total_size\n",
        "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d segundos transcurrido\" %\n",
        "                    (percent, progress_size / (1024.**2), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "import urllib.request\n",
        "url_github_Model='https://github.com/JuanDiegoCastellanos/deep_learning/blob/main/neural_recurrent_network/models/2RNN_BibliaReinaValera1909_model.h5?raw=true'\n",
        "urllib.request.urlretrieve(url_github_Model,\n",
        "                           '2RNN_BibliaReinaValera1909_model.h5', \n",
        "                           reporthook)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% | 60 MB | 3.54 MB/s | 17 segundos transcurrido"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2RNN_BibliaReinaValera1909_model.h5',\n",
              " <http.client.HTTPMessage at 0x7fa4dc827a90>)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2EM_6Nf8pcl"
      },
      "source": [
        "## 18. Instanciación del modelo descargado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NQlI_DI8yvr",
        "outputId": "7380d1f2-7cb0-4985-d2fe-7fd25ca0521f"
      },
      "source": [
        "new_model = tf.keras.models.load_model(filepath='/content/2RNN_BibliaReinaValera1909_model.h5',custom_objects=None)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jAGODbbS_G0q",
        "outputId": "6918638b-c75d-4587-eabe-8c489512941e"
      },
      "source": [
        "df2 = pd.read_csv(\"https://raw.githubusercontent.com/JuanDiegoCastellanos/deep_learning/main/Datasets/data_vocab.csv\")\n",
        "df2.head()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>vocab</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num vocab\n",
              "0    0      \n",
              "1    1     0\n",
              "2    2     1\n",
              "3    3     2\n",
              "4    4     3"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B_UWU8-AEyx"
      },
      "source": [
        "#funcion para generar texto\n",
        "def generate_text(model, start_string):\n",
        "  #definimos cuantos tensores/cantidad de texto generaremos\n",
        "  num_generate=500\n",
        "  #convertimos el texto en números\n",
        "  input_eval  = [char2idx[s] for s in start_string]\n",
        "  input_eval  = tf.expand_dims (input_eval,0)\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 0.2  #(0.0 a  1) entre más alta la temperatura más creatividad al modelo, pero tambien más errores ortograficos.\n",
        "  model.reset_states() #bucle para generar caracteres, mediante predicciones\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval= tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append (idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string+ ''.join(text_generated))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j8Ic-t9AJol",
        "outputId": "feda042d-3cca-4976-95ca-4b4ba76a4f26"
      },
      "source": [
        "print(generate_text(new_model, start_string=u\"inglaterra\"))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inglaterra y en medio de ella sobre su cabeza y como la mar y en la soberbia de los palacios de carro de fuego ardiendo 22 y en el campo de los filisteos habian hecho en la casa de jehova y la casa de jehova y la casa de jacob se ha de purificar la casa de jehova y la casa de jehova se encendio contra el en el tiempo de la vida de su rostro y el respondio he aqui que yo envio sobre vosotros y hare cesar de ti tus piedades y hare segun tus mandamientos y sus manos se han mostrado el rey y sus profetas no s\n"
          ]
        }
      ]
    }
  ]
}